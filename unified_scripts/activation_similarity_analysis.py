#!/usr/bin/env python3
"""
Activation Similarity Analysis Script

This script analyzes the similarity between activations generated by trained models (without ICL) 
and the base model (with ICL) for the same validation examples.

The analysis:
1. Loads the base model and collects activations with ICL for validation data
2. Finds the best performing trained models from each method (tok, act, tna, a2t)
3. Collects activations from trained models without ICL for the same validation data
4. Compares activations using cosine similarity
5. Generates bar plots showing similarity scores for each method

This helps understand how well trained models mimic the base model's ICL behavior.

New features:
- Stores activations as np files for reuse
- Supports multiple eval_datasets, label_types, and num_train_examples
- Runs combinations in parallel using multiprocessing
- Caches activations to avoid recomputation
"""

import os
import json
import torch
import numpy as np
import argparse
import matplotlib.pyplot as plt
from collections import defaultdict
import sys
import glob
from transformers import AutoModelForCausalLM
from peft import PeftModel
from tqdm import tqdm
import time
from functools import partial
import re
from pathlib import Path
import multiprocessing as mp
from multiprocessing import Pool, Manager
import hashlib
from dataclasses import dataclass
from typing import List, Dict, Any, Optional, Tuple

# Set matplotlib backend for headless environments
import matplotlib
matplotlib.use('Agg')

# Import activation collection functions from existing scripts
from train_unified import (
    register_attention_hooks, 
    remove_hooks, 
    get_sorted_attention_layers,
    get_activations
)

# Import evaluation results and hyperparameter functions from plotting scripts
from plot_unified import (
    process_trained_model_results_full,
    find_best_hyperparameters_by_performance,
    find_best_hyperparameters_by_dev_loss,
    load_dev_loss_statistics
)

# Import dataset loading function from evaluation script
from evaluate_batch_optimized import create_or_load_evaluation_dataset

# Import ICLDataset for type hints
from data import ICLDataset

# Import get_model_name from utils for exact model naming
from utils import get_model_name, parse_answer_gsm8k, parse_answer_sciqa, parse_answer_boxed
import pickle


@dataclass
class ActivationAnalysisConfig:
    """Configuration for activation analysis run"""
    model_id: str
    trained_dataset: str
    eval_dataset: str
    icl_source: str
    icl_demos: int
    label_type: str
    hp_selection: str
    lora_type: str
    lora_r: int
    lora_alpha: int
    num_generated_tokens: int
    num_train_examples: int
    run_idx: int
    num_val_examples: int
    device: str
    output_dir: str
    uncertainty: bool = False
    
    def get_config_hash(self) -> str:
        """Generate a unique hash for this configuration"""
        config_str = f"{self.model_id}_{self.trained_dataset}_{self.eval_dataset}_{self.icl_source}_{self.icl_demos}_{self.label_type}_{self.hp_selection}_{self.lora_type}_{self.lora_r}_{self.lora_alpha}_{self.num_generated_tokens}_{self.num_train_examples}_{self.run_idx}_{self.num_val_examples}_{self.device}_{self.uncertainty}"
        return hashlib.md5(config_str.encode()).hexdigest()[:12]


def get_activation_storage_path(config: ActivationAnalysisConfig, model_type: str = None, is_base: bool = False) -> str:
    """Get the storage path for activations based on configuration"""
    base_dir = "../outputs/activations"
    
    if is_base:
        # Base model activations (with ICL)
        path = f"{base_dir}/{config.model_id.split('/')[-1]}/{config.eval_dataset}/{config.icl_source}_{config.icl_demos}/{config.label_type}/{config.num_train_examples}_{config.run_idx}/base_with_icl"
    else:
        # Trained model activations (without ICL)
        path = f"{base_dir}/{config.model_id.split('/')[-1]}/{config.eval_dataset}/{config.icl_source}_{config.icl_demos}/{config.label_type}/{config.num_train_examples}_{config.run_idx}/{model_type}_without_icl"
    
    return path


def save_activations(activations: Dict, config: ActivationAnalysisConfig, model_type: str = None, is_base: bool = False) -> str:
    """Save activations to np files and return the storage path"""
    storage_path = get_activation_storage_path(config, model_type, is_base)
    os.makedirs(storage_path, exist_ok=True)
    
    # Save each example's activations as a separate np file
    for idx, activation in activations.items():
        np_file = os.path.join(storage_path, f"example_{idx}.npy")
        np.save(np_file, activation.float().cpu().numpy())
    
    # Save metadata
    metadata = {
        'num_examples': len(activations),
        'example_indices': list(activations.keys()),
        'config': config.__dict__,
        'model_type': model_type,
        'is_base': is_base,
        'timestamp': time.strftime('%Y-%m-%d %H:%M:%S')
    }
    
    metadata_file = os.path.join(storage_path, "metadata.json")
    with open(metadata_file, 'w') as f:
        json.dump(metadata, f, indent=2, default=str)
    
    print(f"Saved {len(activations)} activations to {storage_path}")
    return storage_path


def load_activations(config: ActivationAnalysisConfig, model_type: str = None, is_base: bool = False) -> Optional[Dict]:
    """Load activations from np files if they exist"""
    storage_path = get_activation_storage_path(config, model_type, is_base)
    metadata_file = os.path.join(storage_path, "metadata.json")
    
    if not os.path.exists(metadata_file):
        return None
    
    try:
        with open(metadata_file, 'r') as f:
            metadata = json.load(f)
        
        # Check if config matches
        saved_config = metadata['config']
        current_config = config.__dict__
        
        # Compare key parameters
        key_params = ['model_id', 'eval_dataset', 'icl_source', 'icl_demos', 'label_type', 
                     'num_train_examples', 'run_idx', 'num_val_examples', 'num_generated_tokens']
        
        for param in key_params:
            if str(saved_config.get(param)) != str(current_config.get(param)):
                print(f"Config mismatch for {param}: {saved_config.get(param)} vs {current_config.get(param)}")
                return None
        
        # Load activations
        activations = {}
        for idx in metadata['example_indices']:
            np_file = os.path.join(storage_path, f"example_{idx}.npy")
            if os.path.exists(np_file):
                activations[int(idx)] = torch.from_numpy(np.load(np_file))
            else:
                print(f"Missing activation file: {np_file}")
                return None
        
        print(f"Loaded {len(activations)} activations from {storage_path}")
        return activations
        
    except Exception as e:
        print(f"Error loading activations from {storage_path}: {e}")
        return None


def save_similarity_results(similarities: Dict, config: ActivationAnalysisConfig, model_type: str) -> str:
    """Save similarity results to pickle file and return the file path"""
    base_dir = "../outputs/activations"
    storage_path = f"{base_dir}/{config.model_id.split('/')[-1]}/{config.eval_dataset}/{config.icl_source}_{config.icl_demos}/{config.label_type}/{config.num_train_examples}_{config.run_idx}/similarities"
    os.makedirs(storage_path, exist_ok=True)
    
    file_path = os.path.join(storage_path, f"{model_type}_similarities.pkl")
    with open(file_path, 'wb') as f:
        pickle.dump(similarities, f)
    
    print(f"Saved similarity results for {model_type} to {file_path}")
    return file_path


def load_similarity_results(config: ActivationAnalysisConfig, model_type: str) -> Optional[Dict]:
    """Load similarity results from pickle file if they exist"""
    base_dir = "../outputs/activations"
    storage_path = f"{base_dir}/{config.model_id.split('/')[-1]}/{config.eval_dataset}/{config.icl_source}_{config.icl_demos}/{config.label_type}/{config.num_train_examples}_{config.run_idx}/similarities"
    file_path = os.path.join(storage_path, f"{model_type}_similarities.pkl")
    
    if not os.path.exists(file_path):
        return None
    
    try:
        with open(file_path, 'rb') as f:
            similarities = pickle.load(f)
        print(f"Loaded similarity results for {model_type} from {file_path}")
        return similarities
    except Exception as e:
        print(f"Error loading similarity results from {file_path}: {e}")
        return None


def save_hyperparameters(hyperparams: Dict, config: ActivationAnalysisConfig) -> str:
    """Save hyperparameters to pickle file and return the file path"""
    base_dir = "../outputs/activations"
    storage_path = f"{base_dir}/{config.model_id.split('/')[-1]}/{config.eval_dataset}/{config.icl_source}_{config.icl_demos}/{config.label_type}/{config.num_train_examples}_{config.run_idx}/hyperparameters"
    os.makedirs(storage_path, exist_ok=True)
    
    file_path = os.path.join(storage_path, "selected_hyperparameters.pkl")
    with open(file_path, 'wb') as f:
        pickle.dump(hyperparams, f)
    
    print(f"Saved hyperparameters to {file_path}")
    return file_path


def load_hyperparameters(config: ActivationAnalysisConfig) -> Optional[Dict]:
    """Load hyperparameters from pickle file if they exist"""
    base_dir = "../outputs/activations"
    storage_path = f"{base_dir}/{config.model_id.split('/')[-1]}/{config.eval_dataset}/{config.icl_source}_{config.icl_demos}/{config.label_type}/{config.num_train_examples}_{config.run_idx}/hyperparameters"
    file_path = os.path.join(storage_path, "selected_hyperparameters.pkl")
    
    if not os.path.exists(file_path):
        return None
    
    try:
        with open(file_path, 'rb') as f:
            hyperparams = pickle.load(f)
        print(f"Loaded hyperparameters from {file_path}")
        return hyperparams
    except Exception as e:
        print(f"Error loading hyperparameters from {file_path}: {e}")
        return None


def check_complete_analysis_exists(config: ActivationAnalysisConfig) -> bool:
    """Check if a complete analysis already exists for this configuration"""
    print(f"Checking for existing analysis: {config.eval_dataset}, {config.label_type}, N={config.num_train_examples}, run={config.run_idx}")
    
    # Check if base activations exist
    base_activations = load_activations(config, is_base=True)
    if base_activations is None:
        print("  - Base activations: MISSING")
        return False
    else:
        print("  - Base activations: FOUND")
    
    # Check if hyperparameters exist
    hyperparams = load_hyperparameters(config)
    if hyperparams is None:
        print("  - Hyperparameters: MISSING")
        return False
    else:
        print("  - Hyperparameters: FOUND")
    
    # Check if all model type similarities exist
    missing_models = []
    for model_type in ['tok', 'act', 'tna', 'a2t']:
        similarities = load_similarity_results(config, model_type)
        if similarities is None:
            missing_models.append(model_type)
        else:
            print(f"  - {model_type} similarities: FOUND")
    
    if missing_models:
        print(f"  - Missing similarities for: {missing_models}")
        return False
    
    print(f"  ✅ Complete analysis already exists!")
    return True


def run_single_analysis_with_reuse_check(config: ActivationAnalysisConfig, reuse_activations: bool = True, force_recompute: bool = False) -> Dict[str, Any]:
    """Run analysis with comprehensive reuse checking"""
    # If reuse is enabled and not forcing recompute, check if complete analysis exists
    if reuse_activations and not force_recompute and check_complete_analysis_exists(config):
        print(f"Skipping analysis - complete results already exist for {config.eval_dataset}, {config.label_type}, N={config.num_train_examples}, run={config.run_idx}")
        
        # Load all existing results
        base_activations = load_activations(config, is_base=True)
        hyperparams = load_hyperparameters(config)
        similarity_results = {}
        
        for model_type in ['tok', 'act', 'tna', 'a2t']:
            similarities = load_similarity_results(config, model_type)
            if similarities is not None:
                similarity_results[model_type] = similarities
        
        return {
            'config': config,
            'similarity_results': similarity_results,
            'hyperparameters': hyperparams
        }
    
    # Otherwise, run the full analysis
    if force_recompute:
        print(f"Force recomputing analysis for {config.eval_dataset}, {config.label_type}, N={config.num_train_examples}, run={config.run_idx}")
    
    return run_single_analysis(config, reuse_activations)


# Create wrapper functions that use the imported get_activations function
def collect_activations_with_icl(model, dataset, tokenizer, device, num_generated_tokens=1):
    """Collect activations from model with ICL demos using imported function"""
    activations_data = get_activations(
        model, dataset, tokenizer, device, 
        num_generated_tokens=num_generated_tokens, 
        get_with_icl=True, 
        get_without_icl=False
    )
    # Extract just the activations from the returned data structure
    result = {}
    for idx, data in activations_data['with_icl'].items():
        if data and 'activations' in data:
            result[idx] = data['activations']
    return result


def collect_activations_without_icl(model, dataset, tokenizer, device, num_generated_tokens=1):
    """Collect activations from model without ICL demos using imported function"""
    activations_data = get_activations(
        model, dataset, tokenizer, device, 
        num_generated_tokens=num_generated_tokens, 
        get_with_icl=False, 
        get_without_icl=True
    )
    # Extract just the activations from the returned data structure
    result = {}
    for idx, data in activations_data['without_icl'].items():
        if data and 'activations' in data:
            result[idx] = data['activations']
    return result


def calculate_layerwise_similarity(activations1, activations2):
    """Calculate cosine similarity for each layer separately"""
    if activations1 is None or activations2 is None:
        return None
    
    # activations shape: [num_layers, num_tokens, hidden_size]
    num_layers = activations1.shape[0]
    similarities = []
    
    for layer_idx in range(num_layers):
        layer_act1 = activations1[layer_idx]  # [num_tokens, hidden_size]
        layer_act2 = activations2[layer_idx]  # [num_tokens, hidden_size]
        
        # Flatten each layer's activations
        flat1 = layer_act1.flatten()
        flat2 = layer_act2.flatten()
        
        # Calculate cosine similarity for this layer
        cos_sim = torch.nn.functional.cosine_similarity(flat1.unsqueeze(0), flat2.unsqueeze(0), dim=1)
        similarities.append(cos_sim.item())
    
    return similarities


def analyze_activation_similarity(base_activations, trained_activations, num_generated_tokens):
    """Analyze activation similarity between base and trained models"""

    similarities = {
        'overall': [],
        'layerwise': [],
        'tokenwise': [],
        'min_max': num_generated_tokens
    }

    min_max = num_generated_tokens
    
    # Calculate overall similarity for each example
    for idx in base_activations.keys():

        # Sometimes the base and trained activations have different number of tokens, so we need to take the minimum.
        max_tokens = min(base_activations[idx].shape[1], trained_activations[idx].shape[1])
        min_max = min(min_max, max_tokens)

        similarity = torch.nn.functional.cosine_similarity(base_activations[idx][:, :max_tokens, :], trained_activations[idx][:, :max_tokens, :], dim=2)
        
        similarities['overall'].append(similarity.mean().float().item())

        similarities['layerwise'].append(similarity.mean(1).float().numpy())
        similarities['tokenwise'].append(similarity.cpu().float().numpy())

        similarities['min_max'] = min_max
    
    return similarities


def plot_similarity_results_aggregated(aggregated_results, output_dir, eval_dataset, trained_dataset, model_id, num_generated_tokens, num_val_examples):
    """Create aggregated plots for activation similarity results across all N values and run indices."""
    os.makedirs(output_dir, exist_ok=True)
    
    # Extract N values and methods
    n_values = sorted(aggregated_results.keys())
    methods = ['tok', 'act', 'tna', 'a2t']
    
    # Create overall similarity bar plot with all N values (averaged across runs)
    plt.figure(figsize=(14, 8))
    
    # Set up bar positions
    x = np.arange(len(methods))
    width = 0.8 / len(n_values)  # Adjust bar width based on number of N values
    
    # Colors for different N values
    colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b']
    
    # Plot bars for each N value
    for i, n_examples in enumerate(n_values):
        means = []
        stds = []
        
        for method in methods:
            if method in aggregated_results[n_examples] and aggregated_results[n_examples][method]['overall']:
                means.append(np.mean(aggregated_results[n_examples][method]['overall']))
                stds.append(np.std(aggregated_results[n_examples][method]['overall']))
            else:
                means.append(0)
                stds.append(0)
        
        # Calculate bar positions
        bar_positions = x + i * width - (len(n_values) - 1) * width / 2
        
        # Create bars with error bars
        bars = plt.bar(bar_positions, means, width, yerr=stds, capsize=3, 
                      alpha=0.8, color=colors[i % len(colors)], 
                      label=f'N={n_examples}', edgecolor='black', linewidth=0.5)
        
        # Add value labels on bars
        for bar, mean, std in zip(bars, means, stds):
            if mean > 0:  # Only add labels for valid data
                height = bar.get_height()
                plt.text(bar.get_x() + bar.get_width()/2., height + std + 0.01,
                        f'{mean:.3f}', ha='center', va='bottom', fontsize=8)
    
    plt.title(f'Activation Similarity: {model_id.split("/")[-1]} on {eval_dataset}\nTrained on {trained_dataset} (All N values, averaged across runs)', fontsize=14)
    plt.xlabel('Training Method', fontsize=12)
    plt.ylabel('Cosine Similarity', fontsize=12)
    plt.ylim(0, 1)
    plt.xticks(x, methods)
    plt.legend(title='Training Examples (N)', bbox_to_anchor=(1.05, 1), loc='upper left')
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    
    # Save plot
    plot_path = os.path.join(output_dir, f"activation_similarity_aggregated_{eval_dataset}_{trained_dataset}_{model_id.split('/')[-1]}_{num_val_examples}.png")
    plt.savefig(plot_path, dpi=300, bbox_inches='tight')
    plt.close()
    
    print(f"Aggregated similarity plot saved to: {plot_path}")
    
    # Create individual N-value plots for detailed analysis (also averaged across runs)
    for n_examples in n_values:
        similarities_by_method = aggregated_results[n_examples]
        
        # Extract method names and overall similarities for this N value
        valid_methods = []
        overall_similarities = []
        
        for method in methods:
            if method in similarities_by_method and similarities_by_method[method]['overall']:
                valid_methods.append(method)
                overall_similarities.append(similarities_by_method[method]['overall'])
        
        if not valid_methods:
            continue
        
        # Create overall similarity bar plot for this N value
        plt.figure(figsize=(10, 6))
        
        # Calculate mean and std for each method
        means = [np.mean(sims) for sims in overall_similarities]
        stds = [np.std(sims) for sims in overall_similarities]
        
        # Create bar plot with error bars
        bars = plt.bar(valid_methods, means, yerr=stds, capsize=5, alpha=0.7, 
                      color=['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728'])
        
        plt.title(f'Activation Similarity: {model_id.split("/")[-1]} on {eval_dataset}\nTrained on {trained_dataset} (N={n_examples}, {num_generated_tokens} tokens, averaged across runs)', fontsize=14)
        plt.xlabel('Training Method', fontsize=12)
        plt.ylabel('Cosine Similarity', fontsize=12)
        plt.ylim(0, 1)
        
        # Add value labels on bars
        for bar, mean, std in zip(bars, means, stds):
            height = bar.get_height()
            plt.text(bar.get_x() + bar.get_width()/2., height + std + 0.01,
                    f'{mean:.3f}±{std:.3f}', ha='center', va='bottom', fontsize=10)
        
        plt.grid(True, alpha=0.3)
        plt.tight_layout()
        
        # Save plot
        plot_path = os.path.join(output_dir, f"activation_similarity_N{n_examples}_aggregated_{eval_dataset}_{trained_dataset}_{model_id.split('/')[-1]}_{num_val_examples}.png")
        plt.savefig(plot_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"N={n_examples} aggregated similarity plot saved to: {plot_path}")
        
        # Create layer-wise similarity heatmap if available
        if any('layerwise' in similarities_by_method[method] and similarities_by_method[method]['layerwise'] is not None 
               for method in valid_methods):
            plt.figure(figsize=(12, 8))
            
            # Collect layer-wise data
            layer_data = []
            for method in valid_methods:
                layer_data.append(np.mean(similarities_by_method[method]['layerwise'], axis=0))
            
            layer_data = np.array(layer_data)
            
            # Create heatmap
            im = plt.imshow(layer_data, cmap='viridis', aspect='auto')
            plt.colorbar(im, label='Cosine Similarity')
            
            plt.title(f'Layer-wise Activation Similarity: {model_id.split("/")[-1]} on {eval_dataset}\nTrained on {trained_dataset} (N={n_examples}, {num_generated_tokens} tokens, averaged across runs)', fontsize=14)
            plt.xlabel('Layer Index', fontsize=12)
            plt.ylabel('Training Method', fontsize=12)
            plt.yticks(range(len(valid_methods)), valid_methods)
            plt.xticks(range(layer_data.shape[1]), [f'L{i}' for i in range(layer_data.shape[1])])
            
            # Add value annotations
            for i in range(len(valid_methods)):
                for j in range(layer_data.shape[1]):
                    plt.text(j, i, f'{layer_data[i, j]:.3f}', ha='center', va='center', 
                            color='white' if layer_data[i, j] < 0.5 else 'black', fontsize=8)
            
            plt.tight_layout()
            
            # Save plot
            plot_path = os.path.join(output_dir, f"activation_similarity_layerwise_N{n_examples}_aggregated_{eval_dataset}_{trained_dataset}_{model_id.split('/')[-1]}_{num_val_examples}.png")
            plt.savefig(plot_path, dpi=300, bbox_inches='tight')
            plt.close()
            
            print(f"N={n_examples} aggregated layer-wise similarity plot saved to: {plot_path}")
        
        # Create individual token-wise similarity heatmaps for each method (aggregated)
        # Only create token-wise plots when num_generated_tokens > 1
        if num_generated_tokens > 1:
            for method in valid_methods:
                if 'tokenwise' in similarities_by_method[method] and similarities_by_method[method]['tokenwise'] is not None:
                    # Get tokenwise data for this method (shape: num_examples x num_layers x num_tokens)
                    tokenwise_data = similarities_by_method[method]['tokenwise']
                    
                    # Average across examples to get (num_layers x num_tokens)
                    tokenwise_mean = np.mean(tokenwise_data, axis=0)
                    
                    # Create individual plot for this method
                    plt.figure(figsize=(24, 6))
                    
                    # Create 2D heatmap: layers x tokens (tokens on x-axis, layers on y-axis)
                    im = plt.imshow(tokenwise_mean, cmap='viridis', aspect='auto', vmin=0.0, vmax=1.0)
                    plt.colorbar(im, label='Cosine Similarity')
                    
                    plt.title(f'Token-wise Activation Similarity: {method.upper()}\n{model_id.split("/")[-1]} on {eval_dataset} (N={n_examples}, averaged across runs)', fontsize=14)
                    plt.xlabel('Token Position', fontsize=12)
                    plt.ylabel('Layer', fontsize=12)
                    
                    # Set axis labels
                    plt.xticks(range(tokenwise_mean.shape[1]), [f'T{i+1}' for i in range(tokenwise_mean.shape[1])])
                    plt.yticks(range(tokenwise_mean.shape[0]), [f'L{i+1}' for i in range(tokenwise_mean.shape[0])])
                    
                    # Add value annotations (only for smaller heatmaps to avoid clutter)
                    if tokenwise_mean.shape[0] <= 12 and tokenwise_mean.shape[1] <= 20:
                        for i in range(tokenwise_mean.shape[0]):
                            for j in range(tokenwise_mean.shape[1]):
                                plt.text(i, j, f'{tokenwise_mean[i, j]:.3f}', ha='center', va='center', 
                                        color='white' if tokenwise_mean[i, j] < 0.5 else 'black', fontsize=8)
                    
                    plt.tight_layout()
                    
                    # Save individual plot
                    plot_path = os.path.join(output_dir, f"activation_similarity_tokenwise_{method}_N{n_examples}_aggregated_{eval_dataset}_{trained_dataset}_{model_id.split('/')[-1]}_{num_val_examples}.png")
                    plt.savefig(plot_path, dpi=300, bbox_inches='tight')
                    plt.close()
                    
                    print(f"N={n_examples} aggregated token-wise similarity plot for {method} saved to: {plot_path}")


def plot_similarity_results_combined(n_similarities, output_dir, eval_dataset, trained_dataset, model_id, num_generated_tokens, num_val_examples):
    """Create combined plots for activation similarity results across all N values."""
    os.makedirs(output_dir, exist_ok=True)
    
    # Extract N values and methods
    n_values = sorted(n_similarities.keys())
    methods = ['tok', 'act', 'tna', 'a2t']
    
    # Create overall similarity bar plot with all N values
    plt.figure(figsize=(14, 8))
    
    # Set up bar positions
    x = np.arange(len(methods))
    width = 0.8 / len(n_values)  # Adjust bar width based on number of N values
    
    # Colors for different N values
    colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b']
    
    # Plot bars for each N value
    for i, n_examples in enumerate(n_values):
        means = []
        stds = []
        
        for method in methods:
            if method in n_similarities[n_examples] and n_similarities[n_examples][method]['overall']:
                means.append(np.mean(n_similarities[n_examples][method]['overall']))
                stds.append(np.std(n_similarities[n_examples][method]['overall']))
            else:
                means.append(0)
                stds.append(0)
        
        # Calculate bar positions
        bar_positions = x + i * width - (len(n_values) - 1) * width / 2
        
        # Create bars with error bars
        bars = plt.bar(bar_positions, means, width, yerr=stds, capsize=3, 
                      alpha=0.8, color=colors[i % len(colors)], 
                      label=f'N={n_examples}', edgecolor='black', linewidth=0.5)
        
        # Add value labels on bars
        for bar, mean, std in zip(bars, means, stds):
            if mean > 0:  # Only add labels for valid data
                height = bar.get_height()
                plt.text(bar.get_x() + bar.get_width()/2., height + std + 0.01,
                        f'{mean:.3f}', ha='center', va='bottom', fontsize=8)
    
    plt.title(f'Activation Similarity: {model_id.split("/")[-1]} on {eval_dataset}\nTrained on {trained_dataset} (All N values)', fontsize=14)
    plt.xlabel('Training Method', fontsize=12)
    plt.ylabel('Cosine Similarity', fontsize=12)
    plt.ylim(0, 1)
    plt.xticks(x, methods)
    plt.legend(title='Training Examples (N)', bbox_to_anchor=(1.05, 1), loc='upper left')
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    
    # Save plot
    plot_path = os.path.join(output_dir, f"activation_similarity_combined_{eval_dataset}_{trained_dataset}_{model_id.split('/')[-1]}_num_val_examples_{num_val_examples}.png")
    plt.savefig(plot_path, dpi=300, bbox_inches='tight')
    plt.close()
    
    print(f"Combined similarity plot saved to: {plot_path}")
    
    # Create individual N-value plots for detailed analysis
    for n_examples in n_values:
        similarities_by_method = n_similarities[n_examples]
        
        # Extract method names and overall similarities for this N value
        valid_methods = []
        overall_similarities = []
        
        for method in methods:
            if method in similarities_by_method and similarities_by_method[method]['overall']:
                valid_methods.append(method)
                overall_similarities.append(similarities_by_method[method]['overall'])
        
        if not valid_methods:
            continue
        
        # Create overall similarity bar plot for this N value
        plt.figure(figsize=(10, 6))
        
        # Calculate mean and std for each method
        means = [np.mean(sims) for sims in overall_similarities]
        stds = [np.std(sims) for sims in overall_similarities]
        
        # Create bar plot with error bars
        bars = plt.bar(valid_methods, means, yerr=stds, capsize=5, alpha=0.7, 
                      color=['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728'])
        
        plt.title(f'Activation Similarity: {model_id.split("/")[-1]} on {eval_dataset}\nTrained on {trained_dataset} (N={n_examples}, {num_generated_tokens} tokens)', fontsize=14)
        plt.xlabel('Training Method', fontsize=12)
        plt.ylabel('Cosine Similarity', fontsize=12)
        plt.ylim(0, 1)
        
        # Add value labels on bars
        for bar, mean, std in zip(bars, means, stds):
            height = bar.get_height()
            plt.text(bar.get_x() + bar.get_width()/2., height + std + 0.01,
                    f'{mean:.3f}±{std:.3f}', ha='center', va='bottom', fontsize=10)
        
        plt.grid(True, alpha=0.3)
        plt.tight_layout()
        
        # Save plot
        plot_path = os.path.join(output_dir, f"activation_similarity_N{n_examples}_{eval_dataset}_{trained_dataset}_{model_id.split('/')[-1]}.png")
        plt.savefig(plot_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"N={n_examples} similarity plot saved to: {plot_path}")
        
        # Create layer-wise similarity heatmap if available
        if any('layerwise' in similarities_by_method[method] and similarities_by_method[method]['layerwise'] is not None 
               for method in valid_methods):
            plt.figure(figsize=(12, 8))
            
            # Collect layer-wise data
            layer_data = []
            for method in valid_methods:
                layer_data.append(np.mean(similarities_by_method[method]['layerwise'], axis=0))
            
            layer_data = np.array(layer_data)
            
            # Create heatmap
            im = plt.imshow(layer_data, cmap='viridis', aspect='auto')
            plt.colorbar(im, label='Cosine Similarity')
            
            plt.title(f'Layer-wise Activation Similarity: {model_id.split("/")[-1]} on {eval_dataset}\nTrained on {trained_dataset} (N={n_examples}, {num_generated_tokens} tokens)', fontsize=14)
            plt.xlabel('Layer Index', fontsize=12)
            plt.ylabel('Training Method', fontsize=12)
            plt.yticks(range(len(valid_methods)), valid_methods)
            plt.xticks(range(layer_data.shape[1]), [f'L{i}' for i in range(layer_data.shape[1])])
            
            # Add value annotations
            for i in range(len(valid_methods)):
                for j in range(layer_data.shape[1]):
                    plt.text(j, i, f'{layer_data[i, j]:.3f}', ha='center', va='center', 
                            color='white' if layer_data[i, j] < 0.5 else 'black', fontsize=8)
            
            plt.tight_layout()
            
            # Save plot
            plot_path = os.path.join(output_dir, f"activation_similarity_layerwise_N{n_examples}_{eval_dataset}_{trained_dataset}_{model_id.split('/')[-1]}.png")
            plt.savefig(plot_path, dpi=300, bbox_inches='tight')
            plt.close()
            
            print(f"N={n_examples} layer-wise similarity plot saved to: {plot_path}")
        
        # Create individual token-wise similarity heatmaps for each method
        # Only create token-wise plots when num_generated_tokens > 1
        if num_generated_tokens > 1:
            for method in valid_methods:
                if 'tokenwise' in similarities_by_method[method] and similarities_by_method[method]['tokenwise'] is not None:
                    # Get tokenwise data for this method (shape: num_examples x num_layers x num_tokens)
                    tokenwise_data = similarities_by_method[method]['tokenwise']
                    
                    # Average across examples to get (num_layers x num_tokens)
                    tokenwise_mean = np.mean(tokenwise_data, axis=0)
                    
                    # Create individual plot for this method
                    plt.figure(figsize=(24, 6))
                    
                    # Create 2D heatmap: layers x tokens (tokens on x-axis, layers on y-axis)
                    im = plt.imshow(tokenwise_mean, cmap='viridis', aspect='auto', vmin=0.0, vmax=1.0)
                    plt.colorbar(im, label='Cosine Similarity')
                    
                    plt.title(f'Token-wise Activation Similarity: {method.upper()}\n{model_id.split("/")[-1]} on {eval_dataset} (N={n_examples})', fontsize=14)
                    plt.xlabel('Token Position', fontsize=12)
                    plt.ylabel('Layer', fontsize=12)
                    
                    # Set axis labels
                    plt.xticks(range(tokenwise_mean.shape[1]), [f'T{i+1}' for i in range(tokenwise_mean.shape[1])])
                    plt.yticks(range(tokenwise_mean.shape[0]), [f'L{i+1}' for i in range(tokenwise_mean.shape[0])])
                    
                    # Add value annotations (only for smaller heatmaps to avoid clutter)
                    if tokenwise_mean.shape[0] <= 12 and tokenwise_mean.shape[1] <= 20:
                        for i in range(tokenwise_mean.shape[0]):
                            for j in range(tokenwise_mean.shape[1]):
                                plt.text(i, j, f'{tokenwise_mean[i, j]:.3f}', ha='center', va='center', 
                                        color='white' if tokenwise_mean[i, j] < 0.5 else 'black', fontsize=8)
                    
                    plt.tight_layout()
                    
                    # Save individual plot
                    plot_path = os.path.join(output_dir, f"activation_similarity_tokenwise_{method}_N{n_examples}_{eval_dataset}_{trained_dataset}_{model_id.split('/')[-1]}.png")
                    plt.savefig(plot_path, dpi=300, bbox_inches='tight')
                    plt.close()
                    
                    print(f"N={n_examples} token-wise similarity plot for {method} saved to: {plot_path}")


def plot_similarity_results(similarities_by_method, output_dir, eval_dataset, trained_dataset, model_id, num_generated_tokens):
    """Create plots for activation similarity analysis"""
    os.makedirs(output_dir, exist_ok=True)
    
    # ============ PLOT 1: Overall similarity comparison ============
    plt.figure(figsize=(12, 8))
    
    methods = list(similarities_by_method.keys())
    overall_similarities = [np.mean(similarities_by_method[method]['overall']) for method in methods]
    overall_stds = [np.std(similarities_by_method[method]['overall']) for method in methods]
    
    # Create bar plot
    x_pos = np.arange(len(methods))
    bars = plt.bar(x_pos, overall_similarities, yerr=overall_stds, 
                   capsize=5, alpha=0.7, color='skyblue', edgecolor='navy')
    
    # Customize plot
    plt.xlabel('Training Method')
    plt.ylabel('Cosine Similarity')
    plt.title(f'Activation Similarity: Trained Models vs Base Model (with ICL)\n'
              f'Dataset: {eval_dataset}, Trained on: {trained_dataset}, Model: {model_id.split("/")[-1]}')
    plt.xticks(x_pos, methods)
    plt.ylim(0, 1.05)
    plt.grid(True, alpha=0.3)
    
    # Add value labels on bars
    for i, (bar, sim, std) in enumerate(zip(bars, overall_similarities, overall_stds)):
        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                f'{sim:.3f}\n±{std:.3f}', ha='center', va='bottom', fontsize=10)
    
    plt.tight_layout()
    
    plot_filename = f"activation_similarity_overall_{eval_dataset}_{trained_dataset}_{model_id.split('/')[-1]}.png"
    plot_path = os.path.join(output_dir, plot_filename)
    plt.savefig(plot_path, dpi=300, bbox_inches='tight')
    plt.close()
    print(f"Overall similarity plot saved to {plot_path}")
    
    # ============ PLOT 2: Layer-wise similarity ============
    plt.figure(figsize=(15, 10))
    
    # Get number of layers from first method
    first_method = methods[0]
    if similarities_by_method[first_method]['layerwise']:
        num_layers = len(similarities_by_method[first_method]['layerwise'][0])
        layer_indices = list(range(num_layers))
        
        # Calculate mean and std for each layer across all examples
        layer_similarities = {}
        for method in methods:
            layer_sims = similarities_by_method[method]['layerwise']
            if layer_sims:
                # Transpose to get [num_layers, num_examples]
                layer_sims_array = np.array(layer_sims).T
                layer_similarities[method] = {
                    'mean': np.mean(layer_sims_array, axis=1),
                    'std': np.std(layer_sims_array, axis=1)
                }
        
        # Plot each method
        for i, method in enumerate(methods):
            if method in layer_similarities:
                means = layer_similarities[method]['mean']
                stds = layer_similarities[method]['std']
                
                plt.errorbar(layer_indices, means, yerr=stds, 
                           marker='o', linestyle='-', label=method, 
                           capsize=3, capthick=1, alpha=0.8)
        
        plt.xlabel('Layer Index')
        plt.ylabel('Cosine Similarity')
        plt.title(f'Layer-wise Activation Similarity\n'
                  f'Dataset: {eval_dataset}, Trained on: {trained_dataset}, Model: {model_id.split("/")[-1]}')
        plt.legend()
        plt.grid(True, alpha=0.3)
        plt.ylim(0, 1.05)
        plt.tight_layout()
        
        plot_filename = f"activation_similarity_layerwise_{eval_dataset}_{trained_dataset}_{model_id.split('/')[-1]}.png"
        plot_path = os.path.join(output_dir, plot_filename)
        plt.savefig(plot_path, dpi=300, bbox_inches='tight')
        plt.close()
        print(f"Layer-wise similarity plot saved to {plot_path}")
    
    # ============ PLOT 3: Token-wise similarity (if multi-token) ============
    if num_generated_tokens > 1:
        plt.figure(figsize=(12, 8))
        
        token_indices = list(range(num_generated_tokens))
        
        # Calculate mean and std for each token across all examples
        token_similarities = {}
        for method in methods:
            token_sims = similarities_by_method[method]['tokenwise']
            if token_sims:
                # Transpose to get [num_tokens, num_examples]
                token_sims_array = np.array(token_sims).T
                token_similarities[method] = {
                    'mean': np.mean(token_sims_array, axis=1),
                    'std': np.std(token_sims_array, axis=1)
                }
        
        # Plot each method
        for i, method in enumerate(methods):
            if method in token_similarities:
                means = token_similarities[method]['mean']
                stds = token_similarities[method]['std']
                
                plt.errorbar(token_indices, means, yerr=stds, 
                           marker='o', linestyle='-', label=method, 
                           capsize=3, capthick=1, alpha=0.8)
        
        plt.xlabel('Token Position')
        plt.ylabel('Cosine Similarity')
        plt.title(f'Token-wise Activation Similarity\n'
                  f'Dataset: {eval_dataset}, Trained on: {trained_dataset}, Model: {model_id.split("/")[-1]}')
        plt.legend()
        plt.grid(True, alpha=0.3)
        plt.ylim(0, 1.05)
        plt.tight_layout()
        
        plot_filename = f"activation_similarity_tokenwise_{eval_dataset}_{trained_dataset}_{model_id.split('/')[-1]}.png"
        plot_path = os.path.join(output_dir, plot_filename)
        plt.savefig(plot_path, dpi=300, bbox_inches='tight')
        plt.close()
        print(f"Token-wise similarity plot saved to {plot_path}")
    else:
        print(f"Skipping token-wise plots for single token generation (num_generated_tokens={num_generated_tokens})")


def run_single_analysis(config: ActivationAnalysisConfig, reuse_activations: bool = True) -> Dict[str, Any]:
    """Run activation similarity analysis for a single configuration"""
    print(f"\n{'='*60}")
    print(f"Running analysis for: {config.eval_dataset}, {config.label_type}, N={config.num_train_examples}, run={config.run_idx}")
    print(f"{'='*60}")
    
    # Setup device and dtype
    device = torch.device(config.device if torch.cuda.is_available() else "cpu")
    torch_dtype = torch.bfloat16
    
    # Load tokenizer
    from transformers import AutoTokenizer
    tokenizer = AutoTokenizer.from_pretrained(config.model_id)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    # Setup parse answer function
    parse_answer_func = None
    if config.eval_dataset == 'gsm8k' or config.eval_dataset == 'gsm8ks':
        parse_answer_func = parse_answer_gsm8k
    elif config.eval_dataset == 'sciqa' or config.eval_dataset == 'strategyreason':
        parse_answer_func = parse_answer_sciqa
    elif 'hmath' in config.eval_dataset:
        parse_answer_func = parse_answer_boxed
    
    best_hp_key = 'without_icl_accuracy_top1' if config.num_generated_tokens == 1 else 'without_icl_accuracy'
    
    # Step 1: Find best hyperparameters for each training variant
    print("Finding best hyperparameters for each training variant...")
    all_selected_models = {}
    all_best_hyperparams = {}
    
    for model_type in ['tok', 'act', 'tna', 'a2t']:
        print(f"\nProcessing {model_type}...")
        
        # Use process_trained_model_results_full to get results and best hyperparameters
        method_data, best_hps, available_metrics = process_trained_model_results_full(
            model_id=config.model_id,
            base_dir="../outputs/evaluations",
            model_type=model_type,
            trained_dataset=config.trained_dataset,
            eval_dataset=config.eval_dataset,
            icl_source=config.icl_source,
            icl_demos=config.icl_demos,
            uncertainty_mode=config.uncertainty,
            label_type=config.label_type,
            hp_selection=config.hp_selection
        )

        if not method_data:
            print(f"No evaluation results found for {model_type}")
            continue
        
        all_selected_models[model_type] = method_data
        all_best_hyperparams[model_type] = best_hps
        print(f"Selected {len(method_data)} best configurations for {model_type}")
    
    if not all_selected_models:
        print("No models found for any training variant!")
        return {}
    
    # Check if this N value exists in the selected models
    if config.num_train_examples not in all_selected_models.get('tok', {}):
        print(f"N={config.num_train_examples} not available for any model type, skipping...")
        return {}
    
    # Load validation dataset
    print(f"Loading validation dataset: {config.eval_dataset} for run {config.run_idx}, N={config.num_train_examples}")
    dataset, cache_path = create_or_load_evaluation_dataset(
        eval_dataset_name=config.eval_dataset,
        icl_source_dataset=config.icl_source,
        run_idx=config.run_idx,
        num_training_examples=config.num_train_examples,
        max_demos=min(config.icl_demos, config.num_train_examples - 1),
        tokenizer=tokenizer,
        num_generated_tokens=config.num_generated_tokens,
        force_rebuild=False,
        parse_answer_func=parse_answer_func
    )

    dataset = dataset[:config.num_val_examples]
    
    if dataset is None:
        print(f"Failed to load validation dataset for run {config.run_idx}, N={config.num_train_examples}!")
        return {}
    
    print(f"Dataset loaded with {len(dataset)} examples for run {config.run_idx}, N={config.num_train_examples}")
    
    # Step 2: Collect or load base model activations with ICL
    print(f"Collecting base model activations with ICL for run {config.run_idx}, N={config.num_train_examples}...")
    
    base_activations = None
    if reuse_activations:
        base_activations = load_activations(config, is_base=True)
    
    if base_activations is None:
        print(f"Loading base model: {config.model_id}")
        base_model = AutoModelForCausalLM.from_pretrained(
            config.model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True
        )
        base_model.to(device)
        print("Base model loaded successfully")

        base_activations = collect_activations_with_icl(
            base_model, dataset, tokenizer, device, config.num_generated_tokens
        )

        del base_model
        torch.cuda.empty_cache()
        
        # Save base activations
        save_activations(base_activations, config, is_base=True)

    print(f"Collected activations for {len(base_activations)} examples from base model")
    
    # Step 3: Collect activations from trained models without ICL
    print(f"Collecting activations from trained models without ICL for run {config.run_idx}, N={config.num_train_examples}...")
    trained_activations = {}
    similarity_results = {}
    
    for model_type in ['tok', 'act', 'tna', 'a2t']:
        if model_type not in all_selected_models:
            continue
        
        print(f"\nProcessing {model_type} for run {config.run_idx}, N={config.num_train_examples}...")
        
        # Check if this N value exists in the selected models
        if config.num_train_examples not in all_selected_models[model_type]:
            print(f"N={config.num_train_examples} not available for {model_type}, skipping...")
            continue
        
        # Try to load existing activations first
        model_activations = None
        if reuse_activations:
            model_activations = load_activations(config, model_type, is_base=False)
        
        if model_activations is None:
            # Get best hyperparameters from the new data structure
            best_hps = all_best_hyperparams[model_type][config.num_train_examples]
            
            # Use get_model_name to get the exact model name
            model_name = get_model_name(
                training_method=model_type,
                model_name=config.model_id.split('/')[-1],
                lora_type=config.lora_type,
                lora_r=config.lora_r,
                lora_alpha=config.lora_alpha,
                num_generated_tokens=config.num_generated_tokens,
                num_train_examples=config.num_train_examples,
                lr=best_hps[best_hp_key][0] if model_type == 'tna' else best_hps[best_hp_key],
                run_idx=config.run_idx,
                label_type=config.label_type if model_type in ['tok', 'a2t'] else None,
                ce_loss_weight=best_hps[best_hp_key][1] if model_type == 'tna' else None
            )

            print(f"Loading base model: {config.model_id}")
            base_model = AutoModelForCausalLM.from_pretrained(
                config.model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True
            )
            base_model.to(device)
            print("Base model loaded successfully")
            
            # Construct the full adapter path
            adapter_path = f"../outputs/{model_type}/{config.trained_dataset}/{model_name}"
            
            if not os.path.exists(adapter_path):
                print(f"Adapter path not found: {adapter_path}")
                continue
            
            print(f"Loading adapter from: {adapter_path}")
            
            try:
                trained_model = PeftModel.from_pretrained(base_model, adapter_path)
                trained_model.to(device)
                trained_model = trained_model.merge_and_unload()
            except Exception as e:
                print(f"Failed to load model from {adapter_path}: {e}")
                continue
            
            # Collect activations without ICL
            model_activations = collect_activations_without_icl(
                trained_model, dataset, tokenizer, device, config.num_generated_tokens
            )

            # Save trained model activations
            save_activations(model_activations, config, model_type, is_base=False)
            
            # Clean up
            del trained_model, base_model
            torch.cuda.empty_cache()
        
        trained_activations[model_type] = model_activations
        print(f"Collected activations for {len(model_activations)} examples from {model_type}")
        
        # Step 4: Analyze activation similarity
        print(f"Analyzing activation similarity for {model_type}...")
        
        # Try to load existing similarity results first
        similarities = None
        if reuse_activations:
            similarities = load_similarity_results(config, model_type)
        
        if similarities is None:
            similarities = analyze_activation_similarity(
                base_activations, model_activations, config.num_generated_tokens
            )
            
            # Save similarity results
            save_similarity_results(similarities, config, model_type)
        
        similarity_results[model_type] = similarities
        
        # Print summary statistics
        if similarities['overall']:
            mean_sim = np.mean(similarities['overall'])
            std_sim = np.std(similarities['overall'])
            print(f"  {model_type}: Mean similarity = {mean_sim:.4f} ± {std_sim:.4f}")
    
    # Save hyperparameters
    save_hyperparameters(all_best_hyperparams, config)
    
    # Clean up dataset
    del dataset
    torch.cuda.empty_cache()
    
    return {
        'config': config,
        'similarity_results': similarity_results,
        'hyperparameters': all_best_hyperparams
    }


def main():
    parser = argparse.ArgumentParser(description="Activation similarity analysis between trained models and base model")
    
    # Data configuration
    parser.add_argument("--trained_datasets", nargs='+', default=["sciq_remap"],
                        help="Datasets the models were trained on")
    parser.add_argument("--eval_datasets", nargs='+', default=["sciq_remap", "qasc_remap"],
                        help="Datasets to evaluate on")
    parser.add_argument("--icl_sources", nargs='+', default=["sciq_remap"],
                        help="ICL source datasets")
    parser.add_argument("--label_types", nargs='+', default=["icl_outputs", "ground_truth"],
                        choices=["ground_truth", "icl_outputs"],
                        help="Label types used during training")
    parser.add_argument("--num_generated_tokens", type=int, default=1,
                        help="Number of generated tokens during training/evaluation")
    parser.add_argument("--num_train_examples", nargs='+', default=[2, 4, 8],
                        help="Number of training examples to use for dataset creation")
    parser.add_argument("--run_indices", nargs='+', default=[0, 1, 2, 3, 4],
                        help="Run indices to use for comparison")
    parser.add_argument("--uncertainty", action="store_true", default=False,
                        help="Whether to use uncertainty")
    
    # Model configuration
    parser.add_argument("--model_id", type=str, default="Qwen/Qwen3-4B-Base",
                        help="Base model ID")
    parser.add_argument("--lora_type", type=str, default="qko",
                        help="LoRA type")
    parser.add_argument("--lora_r", type=int, default=8,
                        help="LoRA rank")
    parser.add_argument("--lora_alpha", type=int, default=8,
                        help="LoRA alpha")
    parser.add_argument("--num_val_examples", type=int, default=100,
                        help="Number of validation examples to use")
    
    # Evaluation configuration
    parser.add_argument("--icl_demos", type=int, default=256,
                        help="Number of ICL demos")
    parser.add_argument("--base_dir", type=str, default="../outputs/evaluations",
                        help="Base directory for evaluation results")
    
    # Analysis options
    parser.add_argument("--hp_selection", type=str, choices=['performance', 'dev_loss'], default='performance',
                        help="Hyperparameter selection method")
    parser.add_argument("--device", type=str, default="cuda:0",
                        help="Device to use for computation")
    parser.add_argument("--reuse_activations", action="store_true", default=True,
                        help="Reuse previously calculated activations")
    parser.add_argument("--force_recompute", action="store_true", default=False,
                        help="Force recomputation even if results already exist")
    parser.add_argument("--num_processes", type=int, default=2,
                        help="Number of parallel processes to use")
    
    # Output
    parser.add_argument("--output_dir", type=str, default="../plots/activation_similarity",
                        help="Output directory for plots")
    
    args = parser.parse_args()
    
    # Generate all configuration combinations
    configs = []
    for trained_dataset in args.trained_datasets:
        for eval_dataset in args.eval_datasets:
            for label_type in args.label_types:
                for icl_source in args.icl_sources:
                    for num_train_examples in args.num_train_examples:
                        for run_idx in args.run_indices:
                            config = ActivationAnalysisConfig(
                                model_id=args.model_id,
                                trained_dataset=trained_dataset,
                                eval_dataset=eval_dataset,
                                icl_source=icl_source,
                                icl_demos=args.icl_demos,
                                label_type=label_type,
                                hp_selection=args.hp_selection,
                                lora_type=args.lora_type,
                                lora_r=args.lora_r,
                                lora_alpha=args.lora_alpha,
                                num_generated_tokens=args.num_generated_tokens,
                                num_train_examples=num_train_examples,
                                run_idx=run_idx,
                                num_val_examples=args.num_val_examples,
                                device=args.device,
                                output_dir=args.output_dir,
                                uncertainty=args.uncertainty
                            )
                            configs.append(config)
    
    print(f"Generated {len(configs)} configuration combinations")
    print("Configuration summary:")
    for i, config in enumerate(configs[:5]):  # Show first 5
        print(f"  {i+1}. {config.eval_dataset}, {config.label_type}, N={config.num_train_examples}, run={config.run_idx}")
    if len(configs) > 5:
        print(f"  ... and {len(configs) - 5} more")
    
    # Check what can be reused
    if args.reuse_activations and not args.force_recompute:
        print(f"\nChecking for existing analyses...")
        complete_analyses = 0
        partial_analyses = 0
        new_analyses = 0
        
        for config in configs:
            if check_complete_analysis_exists(config):
                complete_analyses += 1
            else:
                # Check if any components exist
                base_exists = load_activations(config, is_base=True) is not None
                hyperparams_exist = load_hyperparameters(config) is not None
                similarities_exist = any(load_similarity_results(config, model_type) is not None 
                                       for model_type in ['tok', 'act', 'tna', 'a2t'])
                
                if base_exists or hyperparams_exist or similarities_exist:
                    partial_analyses += 1
                else:
                    new_analyses += 1
        
        print(f"\nReuse Summary:")
        print(f"  - Complete analyses (will skip): {complete_analyses}")
        print(f"  - Partial analyses (will reuse some components): {partial_analyses}")
        print(f"  - New analyses (will compute everything): {new_analyses}")
        print(f"  - Total time saved: ~{complete_analyses} full analyses")
    elif args.force_recompute:
        print(f"\nForce recompute mode: All analyses will be recomputed")
    else:
        print(f"\nReuse disabled: All analyses will be computed from scratch")
    
    # Run analyses in parallel
    print(f"\nStarting parallel analysis with {args.num_processes} processes...")
    start_time = time.time()
    
    if args.num_processes == 1:
        # Sequential execution for debugging
        all_results = []
        for config in configs:
            result = run_single_analysis_with_reuse_check(config, args.reuse_activations, args.force_recompute)
            if result:
                all_results.append(result)
    else:
        # Parallel execution
        with Pool(processes=args.num_processes) as pool:
            # Create partial function with reuse_activations and force_recompute arguments
            run_analysis_partial = partial(run_single_analysis_with_reuse_check, 
                                         reuse_activations=args.reuse_activations, 
                                         force_recompute=args.force_recompute)
            
            # Run analyses in parallel
            results = pool.map(run_analysis_partial, configs)
            
            # Filter out empty results
            all_results = [result for result in results if result]
    
    end_time = time.time()
    
    print(f"\nCompleted {len(all_results)} analyses in {end_time - start_time:.1f} seconds")
    
    if not all_results:
        print("No successful analyses completed!")
        return
    
    # Aggregate results for plotting
    print("\nAggregating results for plotting...")
    
    # Group results by eval_dataset, label_type, and trained_dataset for plotting
    grouped_results = defaultdict(lambda: defaultdict(list))
    
    for result in all_results:
        config = result['config']
        key = (config.eval_dataset, config.label_type, config.trained_dataset)
        grouped_results[key][config.num_train_examples].append(result)
    
    # Create plots for each group
    for (eval_dataset, label_type, trained_dataset), n_results in grouped_results.items():
        print(f"\nCreating plots for {eval_dataset}, {label_type}, {trained_dataset}...")
        
        # Aggregate similarities across runs for each N value
        aggregated_results = {}
        for n_examples, results_for_n in n_results.items():
            aggregated_results[n_examples] = {}
            
            # Collect similarities from all runs for this N value
            for model_type in ['tok', 'act', 'tna', 'a2t']:
                all_overall_sims = []
                all_layerwise_sims = []
                all_tokenwise_sims = []
                
                for result in results_for_n:
                    if model_type in result['similarity_results']:
                        similarities = result['similarity_results'][model_type]
                        
                        if similarities['overall']:
                            all_overall_sims.extend(similarities['overall'])
                        
                        if similarities['layerwise'] is not None:
                            all_layerwise_sims.extend(similarities['layerwise'])
                        
                        if similarities['tokenwise'] is not None:
                            all_tokenwise_sims.extend(similarities['tokenwise'])
                
                # Store aggregated results
                aggregated_results[n_examples][model_type] = {
                    'overall': all_overall_sims if all_overall_sims else None,
                    'layerwise': all_layerwise_sims if all_layerwise_sims else None,
                    'tokenwise': all_tokenwise_sims if all_tokenwise_sims else None
                }

        # Create plots
        output_dir = os.path.join(args.output_dir, trained_dataset, label_type)
        plot_similarity_results_aggregated(
            aggregated_results, output_dir, eval_dataset, 
            trained_dataset, args.model_id, args.num_generated_tokens, args.num_val_examples
        )
    
    # Save summary results (without large data)
    print("\nSaving summary results...")
    
    summary_data = {
        'config': {
            'model_id': args.model_id,
            'trained_datasets': args.trained_datasets,
            'eval_datasets': args.eval_datasets,
            'label_types': args.label_types,
            'icl_sources': args.icl_sources,
            'icl_demos': args.icl_demos,
            'hp_selection': args.hp_selection,
            'lora_type': args.lora_type,
            'lora_r': args.lora_r,
            'lora_alpha': args.lora_alpha,
            'num_generated_tokens': args.num_generated_tokens,
            'num_train_examples': args.num_train_examples,
            'run_indices': args.run_indices,
            'num_val_examples': args.num_val_examples,
            'device': args.device,
            'output_dir': args.output_dir,
            'uncertainty': args.uncertainty,
            'reuse_activations': args.reuse_activations,
            'num_processes': args.num_processes
        },
        'analysis_summary': {
            'total_configurations': len(configs),
            'successful_analyses': len(all_results),
            'failed_analyses': len(configs) - len(all_results),
            'success_rate': len(all_results) / len(configs) * 100 if configs else 0,
            'total_time_seconds': end_time - start_time
        },
        'file_paths': {
            'activation_storage_base': "../outputs/activations",
            'similarity_storage_base': "../outputs/activations",
            'hyperparameter_storage_base': "../outputs/activations"
        },
        'completed_configurations': []
    }
    
    # Add completed configuration summaries
    for result in all_results:
        config = result['config']
        summary_data['completed_configurations'].append({
            'eval_dataset': config.eval_dataset,
            'trained_dataset': config.trained_dataset,
            'label_type': config.label_type,
            'icl_source': config.icl_source,
            'num_train_examples': config.num_train_examples,
            'run_idx': config.run_idx,
            'config_hash': config.get_config_hash(),
            'activation_paths': {
                'base_activations': get_activation_storage_path(config, is_base=True),
                'trained_activations': {
                    model_type: get_activation_storage_path(config, model_type, is_base=False)
                    for model_type in result['similarity_results'].keys()
                }
            },
            'similarity_paths': {
                model_type: f"{get_activation_storage_path(config, model_type, is_base=False).replace('_without_icl', '/similarities')}/{model_type}_similarities.pkl"
                for model_type in result['similarity_results'].keys()
            },
            'hyperparameter_path': f"{get_activation_storage_path(config, is_base=True).replace('/base_with_icl', '/hyperparameters')}/selected_hyperparameters.pkl"
        })
    
    # Save summary
    timestamp = time.strftime('%Y%m%d_%H%M%S')
    summary_path = os.path.join(args.output_dir, f"activation_similarity_summary_{timestamp}.json")
    os.makedirs(args.output_dir, exist_ok=True)
    
    with open(summary_path, 'w') as f:
        json.dump(summary_data, f, indent=2, default=str)
    
    print(f"Summary saved to: {summary_path}")
    print("Analysis complete!")


if __name__ == "__main__":
    main()
